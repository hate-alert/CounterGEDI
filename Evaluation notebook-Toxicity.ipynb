{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for emotion detection\n",
    "https://github.com/monologg/GoEmotions-pytorch\n",
    "### For toxicity detection\n",
    "perspective api\n",
    "### For politeness detection\n",
    "https://github.com/AlafateABULIMITI/politeness-detection\n",
    "### For language quality\n",
    "https://github.com/WanzhengZhu/GPS/blob/master/language_quality.py\n",
    "### Other metrics are aleady added add two metric from the GPS model.\n",
    "### Implement GPS model as a baseline.\n",
    "save_path='../HULK_new/Counterspeech/Saved_Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "API_KEY = 'AIzaSyAUYdcELBp_FIPyiCQ6UfDEDIfDJDiShLE'\n",
    "\n",
    "client = discovery.build(\n",
    "      \"commentanalyzer\",\n",
    "      \"v1alpha1\",\n",
    "      developerKey=API_KEY,\n",
    "      discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "      static_discovery=False,\n",
    "    )\n",
    "\n",
    "def get_non_toxicity_score(sentence):\n",
    "    analyze_request = {\n",
    "      'comment': { 'text': sentence },\n",
    "      'requestedAttributes': {'TOXICITY': {}}\n",
    "    }\n",
    "    response = client.comments().analyze(body=analyze_request).execute()\n",
    "    tox=response['attributeScores']['TOXICITY']['summaryScore']['value']\n",
    "    time.sleep(1.0)\n",
    "    return 1-tox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04108893999999996"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_non_toxicity_score(\"I hate niggers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLEU, METEOR, Diversity, Novelty\n",
    "import glob\n",
    "import json\n",
    "from loguru import logger\n",
    "import pandas as pd \n",
    "from Generation.eval import *\n",
    "path_datasets = '/home/adarsh-binny' + '/HULK_new/Counterspeech/Datasets'\n",
    "path_result   = '/home/adarsh-binny' + '/HULK_new/Counterspeech/Results'\n",
    "save_path     = '/home/adarsh-binny' + '/HULK_new/Counterspeech/metrics_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(trained_on,tested_on):\n",
    "    train_path = glob.glob(path_datasets+'/*'+trained_on+'*/*rain*')[0]\n",
    "    gen        = glob.glob(path_result+'/*'+trained_on+'*'+tested_on+'*1628*')\n",
    "    ref        = glob.glob(path_result+'/*'+tested_on+'*'+'references'+'*')[0]\n",
    "    \n",
    "#     print(gen,ref)\n",
    "    with open(ref, 'r') as file:\n",
    "        ref_dict   = json.loads(file.read())\n",
    "        \n",
    "    scores = {}\n",
    "    tot = 0\n",
    "    for files in gen:\n",
    "        with open(files, 'r') as file:\n",
    "            gen_dict  = json.loads(file.read())\n",
    "            \n",
    "        emotion   = gen_dict['params']['task_name'][0][1]\n",
    "        gpu_id    = gen_dict['params']['gpu_id']\n",
    "        hypo = []\n",
    "        refs = []\n",
    "        if emotion=='joy' and str(gpu_id)=='1':\n",
    "            for key in gen_dict['samples']:\n",
    "                if tot>100:\n",
    "                    break\n",
    "                for sentences in gen_dict['samples'][key]['counterspeech_model']:\n",
    "                    hypo.append(sentences)\n",
    "                    refs.append(ref_dict['samples'][key]['counterspeech_model'])\n",
    "                tot = tot + 1\n",
    "\n",
    "            train = pd.read_csv(train_path)\n",
    "            train_set = list(zip(train['initiator_message'].tolist(),train['reply_message'].tolist()))\n",
    "            params = [hypo,refs]\n",
    "            bleu, bleu_4, meteor_ = nltk_metrics(params)\n",
    "            train_corpus = training_corpus(train_set)\n",
    "            diversity, novelty = diversity_and_novelty(train_corpus,hypo)\n",
    "            data_dict = {\n",
    "                         'bleu':bleu,\n",
    "                         'bleu_4':bleu_4,\n",
    "                         'diversity':diversity,\n",
    "                         'novelty':novelty, \n",
    "                         'meteor':meteor_\n",
    "                       }\n",
    "            key = trained_on+'_'+tested_on+'_'+emotion+'_'+str(gpu_id)\n",
    "            scores[key] = data_dict\n",
    "            logger.info(f'Key:{key}--bleu :{bleu}--bleu_4:{bleu_4}--diversity:{diversity}--novelty:{novelty}--meteor:{meteor_}')\n",
    "    \n",
    "    json.dump(scores, open(save_path +tested_on+'.json','w'),indent = 4)\n",
    "                    \n",
    "    \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarsh-binny/.conda/envs/test_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/adarsh-binny/.conda/envs/test_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "2021-08-18 16:36:11.414 | INFO     | __main__:f:42 - Key:CONAN_CONAN_toxic_1--bleu :0.5009974364496044--bleu_4:0.5009974364496044--diversity:0.8087049867720634--novelty:0.84259901060552--meteor:0.1670292599359696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 16:36:16.958 | INFO     | __main__:f:42 - Key:CONAN_CONAN_joy_0--bleu :0.5024940427181879--bleu_4:0.5025128542329175--diversity:0.7966439992002184--novelty:0.8368270669465472--meteor:0.17637176053283168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 16:37:48.378 | INFO     | __main__:f:42 - Key:CONAN_CONAN_anger_1--bleu :0.34004115542738955--bleu_4:0.34004115542738955--diversity:0.858698473458461--novelty:0.894585986577637--meteor:0.15102482703699693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 16:37:54.712 | INFO     | __main__:f:42 - Key:CONAN_CONAN_surprise_1--bleu :0.25433399146572544--bleu_4:0.25433399146572544--diversity:0.8457372675365773--novelty:0.8958242464472553--meteor:0.12820873393795015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 16:37:54.879 | INFO     | __main__:f:42 - Key:CONAN_CONAN_joy_1--bleu :0.38790688956093866--bleu_4:0.38790688956093866--diversity:0.8103381803436333--novelty:0.877103428085856--meteor:0.17037657143026955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 16:37:54.911 | INFO     | __main__:f:42 - Key:CONAN_CONAN_love_1--bleu :0.15132575773783022--bleu_4:0.1513409237865077--diversity:0.7954131462122485--novelty:0.8967413179983085--meteor:0.091595073467257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 16:38:24.562 | INFO     | __main__:f:42 - Key:CONAN_CONAN_sadness_0--bleu :0.36060291577376724--bleu_4:0.36060291577376724--diversity:0.817835742407302--novelty:0.8833927203844146--meteor:0.17052843278504146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 16:38:35.295 | INFO     | __main__:f:42 - Key:CONAN_CONAN_fear_0--bleu :0.3628025900977262--bleu_4:0.3628025900977262--diversity:0.7905926672229553--novelty:0.8762991525843769--meteor:0.18265533594843028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "trained_on = 'CONAN'\n",
    "tested_on  = 'CONAN'\n",
    "gen        = glob.glob(path_result+'/*'+trained_on+'*'+tested_on+'*1628*')\n",
    "ref        = glob.glob(path_result+'/*'+tested_on+'*'+'references'+'*')[0]\n",
    "train_path = glob.glob(path_datasets+'/*'+trained_on+'*/*rain*')[0]    \n",
    "\n",
    "#     print(gen,ref)\n",
    "with open(ref, 'r') as file:\n",
    "    ref_dict   = json.loads(file.read())\n",
    "    \n",
    "import time\n",
    "scores = {}\n",
    "from multiprocessing import Pool\n",
    "def f(x):\n",
    "    files = gen[x]\n",
    "    with open(files, 'r') as file:\n",
    "        gen_dict  = json.loads(file.read())\n",
    "    emotion   = gen_dict['params']['task_name'][0][1]\n",
    "    gpu_id    = gen_dict['params']['gpu_id']\n",
    "    hypo = []\n",
    "    refs = []\n",
    "    for key in gen_dict['samples']:\n",
    "        for sentences in gen_dict['samples'][key]['counterspeech_model']:\n",
    "            hypo.append(sentences)\n",
    "            refs.append(ref_dict['samples'][key]['counterspeech_model'])\n",
    "\n",
    "    train = pd.read_csv(train_path)\n",
    "    train_set = list(zip(train['initiator_message'].tolist(),train['reply_message'].tolist()))\n",
    "    params = [hypo,refs]\n",
    "    bleu, bleu_4, meteor_ = nltk_metrics(params)\n",
    "    train_corpus = training_corpus(train_set)\n",
    "    diversity, novelty = diversity_and_novelty(train_corpus,hypo)\n",
    "    data_dict = {\n",
    "                 'bleu':bleu,\n",
    "                 'bleu_4':bleu_4,\n",
    "                 'diversity':diversity,\n",
    "                 'novelty':novelty, \n",
    "                 'meteor':meteor_\n",
    "               }\n",
    "    key = trained_on+'_'+tested_on+'_'+emotion+'_'+str(gpu_id)\n",
    "    scores[key] = data_dict\n",
    "    logger.info(f'Key:{key}--bleu :{bleu}--bleu_4:{bleu_4}--meteor:{meteor_}--novelty:{novelty}--diversity:{diversity}')\n",
    "    print(x*x)\n",
    "p = Pool(len(gen))\n",
    "p.map(f, [x for x in range(len(gen))])\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarsh-binny/.conda/envs/test_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "2021-08-18 17:18:21.703 | INFO     | __main__:f:45 - Key:Reddit_Reddit_toxic_1--bleu :0.2687999571541444--bleu_4:0.2687999571541444--meteor:0.17141656173534173--novelty:0.7997292845439194--diversity:0.7633809702609596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:18:35.220 | INFO     | __main__:f:45 - Key:Reddit_Reddit_polite_0--bleu :0.24689651850085045--bleu_4:0.24689651850085045--meteor:0.17671899100407673--novelty:0.8244810162291814--diversity:0.7555619656728798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:18:42.842 | INFO     | __main__:f:45 - Key:Reddit_Reddit_joy_0--bleu :0.2353384176882342--bleu_4:0.2353384176882342--meteor:0.19557844334385277--novelty:0.8184690971164061--diversity:0.7452483194441122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:18:46.393 | INFO     | __main__:f:45 - Key:Reddit_Reddit_anger_1--bleu :0.19948645741382345--bleu_4:0.19948645741382345--meteor:0.15783335447699784--novelty:0.865301172929225--diversity:0.7990119316454678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:18:57.659 | INFO     | __main__:f:45 - Key:Reddit_Reddit_joy_1--bleu :0.2053442195664613--bleu_4:0.2053442195664613--meteor:0.18219013265405054--novelty:0.8516467229298946--diversity:0.7674732839353133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:18:58.301 | INFO     | __main__:f:45 - Key:Reddit_Reddit_surprise_1--bleu :0.14658787743341956--bleu_4:0.14658787743341956--meteor:0.13934352868517644--novelty:0.8774854199933643--diversity:0.799932488496469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:18:59.766 | INFO     | __main__:f:45 - Key:Reddit_Reddit_love_1--bleu :0.07919407916260741--bleu_4:0.07923296376330191--meteor:0.0835683057083591--novelty:0.8527399168711473--diversity:0.7295574584294015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:19:10.776 | INFO     | __main__:f:45 - Key:Reddit_Reddit_sadness_0--bleu :0.18167749891650575--bleu_4:0.18167749891650575--meteor:0.18534996813724686--novelty:0.8679918599424555--diversity:0.7773334781408139\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:19:12.553 | INFO     | __main__:f:45 - Key:Reddit_Reddit_fear_0--bleu :0.18089967019428677--bleu_4:0.18089967019428677--meteor:0.19188753517992246--novelty:0.8643715656454425--diversity:0.7486228543617595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "trained_on = 'Reddit'\n",
    "tested_on  = 'Reddit'\n",
    "gen        = glob.glob(path_result+'/*'+trained_on+'*'+tested_on+'*1628*')\n",
    "ref        = glob.glob(path_result+'/*'+tested_on+'*'+'references'+'*')[0]\n",
    "train_path = glob.glob(path_datasets+'/*'+trained_on+'*/*rain*')[0]    \n",
    "\n",
    "#     print(gen,ref)\n",
    "with open(ref, 'r') as file:\n",
    "    ref_dict   = json.loads(file.read())\n",
    "    \n",
    "import time\n",
    "scores = {}\n",
    "from multiprocessing import Pool\n",
    "def f(x):\n",
    "    files = gen[x]\n",
    "    with open(files, 'r') as file:\n",
    "        gen_dict  = json.loads(file.read())\n",
    "    emotion   = gen_dict['params']['task_name'][0][1]\n",
    "    gpu_id    = gen_dict['params']['gpu_id']\n",
    "    hypo = []\n",
    "    refs = []\n",
    "    for key in gen_dict['samples']:\n",
    "        for sentences in gen_dict['samples'][key]['counterspeech_model']:\n",
    "            hypo.append(sentences)\n",
    "            refs.append(ref_dict['samples'][key]['counterspeech_model'])\n",
    "\n",
    "    \n",
    "    hypo = hypo[0:1000]\n",
    "    refs = refs[0:1000]\n",
    "    train = pd.read_csv(train_path)\n",
    "    train_set = list(zip(train['initiator_message'].tolist(),train['reply_message'].tolist()))\n",
    "    params = [hypo,refs]\n",
    "    bleu, bleu_4, meteor_ = nltk_metrics(params)\n",
    "    train_corpus = training_corpus(train_set)\n",
    "    diversity, novelty = diversity_and_novelty(train_corpus,hypo)\n",
    "    data_dict = {\n",
    "                 'bleu':bleu,\n",
    "                 'bleu_4':bleu_4,\n",
    "                 'diversity':diversity,\n",
    "                 'novelty':novelty, \n",
    "                 'meteor':meteor_\n",
    "               }\n",
    "    key = trained_on+'_'+tested_on+'_'+emotion+'_'+str(gpu_id)\n",
    "    scores[key] = data_dict\n",
    "    logger.info(f'Key:{key}--bleu :{bleu}--bleu_4:{bleu_4}--meteor:{meteor_}--novelty:{novelty}--diversity:{diversity}')\n",
    "    print(x*x)\n",
    "p = Pool(len(gen))\n",
    "p.map(f, [x for x in range(len(gen))])\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adarsh-binny/.conda/envs/test_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "2021-08-18 17:12:43.434 | INFO     | __main__:f:45 - Key:Gab_Gab_toxic_1--bleu :0.3189405205920289--bleu_4:0.3189405205920289--meteor:0.1975730245905354--novelty:0.7709120561884353--diversity:0.7481308190032024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:13:04.421 | INFO     | __main__:f:45 - Key:Gab_Gab_joy_0--bleu :0.3023790901074404--bleu_4:0.3023790901074404--meteor:0.20628722506565114--novelty:0.7815410368887311--diversity:0.7402570939734515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:13:36.172 | INFO     | __main__:f:45 - Key:Gab_Gab_anger_1--bleu :0.2367587012798933--bleu_4:0.2367587012798933--meteor:0.16726295652532114--novelty:0.8527496418259678--diversity:0.8100655261671662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:14:00.167 | INFO     | __main__:f:45 - Key:Gab_Gab_joy_1--bleu :0.2407592970345933--bleu_4:0.2407592970345933--meteor:0.19489957198585958--novelty:0.8349478343494948--diversity:0.7662762394093243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:14:07.803 | INFO     | __main__:f:45 - Key:Gab_Gab_surprise_1--bleu :0.16081402050937918--bleu_4:0.16081402050937918--meteor:0.14185085638513206--novelty:0.871919296870229--diversity:0.8156932848770058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:14:18.979 | INFO     | __main__:f:45 - Key:Gab_Gab_love_1--bleu :0.09063308470600961--bleu_4:0.09066348243430587--meteor:0.10108872642031558--novelty:0.8331761666911481--diversity:0.6991702330193547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:14:34.666 | INFO     | __main__:f:45 - Key:Gab_Gab_sadness_0--bleu :0.21206847794063147--bleu_4:0.21206847794063147--meteor:0.1895166207878136--novelty:0.8586144041520748--diversity:0.7858398893346482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-18 17:14:35.244 | INFO     | __main__:f:45 - Key:Gab_Gab_fear_0--bleu :0.2112293031992602--bleu_4:0.2112293031992602--meteor:0.20756536470457454--novelty:0.853661139083761--diversity:0.7535702200291107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "trained_on = 'Gab'\n",
    "tested_on  = 'Gab'\n",
    "gen        = glob.glob(path_result+'/*'+trained_on+'*'+tested_on+'*1628*')\n",
    "ref        = glob.glob(path_result+'/*'+tested_on+'*'+'references'+'*')[0]\n",
    "train_path = glob.glob(path_datasets+'/*'+trained_on+'*/*rain*')[0]    \n",
    "\n",
    "#     print(gen,ref)\n",
    "with open(ref, 'r') as file:\n",
    "    ref_dict   = json.loads(file.read())\n",
    "    \n",
    "import time\n",
    "scores = {}\n",
    "from multiprocessing import Pool\n",
    "def f(x):\n",
    "    files = gen[x]\n",
    "    with open(files, 'r') as file:\n",
    "        gen_dict  = json.loads(file.read())\n",
    "    emotion   = gen_dict['params']['task_name'][0][1]\n",
    "    gpu_id    = gen_dict['params']['gpu_id']\n",
    "    hypo = []\n",
    "    refs = []\n",
    "    for key in gen_dict['samples']:\n",
    "        for sentences in gen_dict['samples'][key]['counterspeech_model']:\n",
    "            hypo.append(sentences)\n",
    "            refs.append(ref_dict['samples'][key]['counterspeech_model'])\n",
    "\n",
    "    \n",
    "    hypo = hypo[0:1000]\n",
    "    refs = refs[0:1000]\n",
    "    train = pd.read_csv(train_path)\n",
    "    train_set = list(zip(train['initiator_message'].tolist(),train['reply_message'].tolist()))\n",
    "    params = [hypo,refs]\n",
    "    bleu, bleu_4, meteor_ = nltk_metrics(params)\n",
    "    train_corpus = training_corpus(train_set)\n",
    "    diversity, novelty = diversity_and_novelty(train_corpus,hypo)\n",
    "    data_dict = {\n",
    "                 'bleu':bleu,\n",
    "                 'bleu_4':bleu_4,\n",
    "                 'diversity':diversity,\n",
    "                 'novelty':novelty, \n",
    "                 'meteor':meteor_\n",
    "               }\n",
    "    key = trained_on+'_'+tested_on+'_'+emotion+'_'+str(gpu_id)\n",
    "    scores[key] = data_dict\n",
    "    logger.info(f'Key:{key}--bleu :{bleu}--bleu_4:{bleu_4}--meteor:{meteor_}--novelty:{novelty}--diversity:{diversity}')\n",
    "    print(x*x)\n",
    "p = Pool(len(gen))\n",
    "p.map(f, [x for x in range(len(gen))])\n",
    "p.close()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Reddit****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 14:15:19.397 | INFO     | __main__:metrics:39 - Key:Reddit_Reddit_joy_1--bleu :0.19159508198821035--bleu_4:0.19159508198821035--diversity:0.7714839932389237--novelty:0.8665133391825054--meteor:0.15676525866753221\n",
      "/home/adarsh-binny/.conda/envs/test_env/lib/python3.7/site-packages/nltk/translate/bleu_score.py:516: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "2021-08-11 14:59:40.330 | INFO     | __main__:metrics:39 - Key:Reddit_Reddit_toxic_1--bleu :0.2524448470938123--bleu_4:0.2524601042958655--diversity:0.7596694295375596--novelty:0.817504188579462--meteor:0.15125937613078166\n",
      "2021-08-11 15:53:26.998 | INFO     | __main__:metrics:39 - Key:Reddit_Reddit_joy_0--bleu :0.23484611013826465--bleu_4:0.23485025476819776--diversity:0.7425532033954653--novelty:0.8235129741410291--meteor:0.1708030453337513\n",
      "2021-08-11 16:59:00.615 | INFO     | __main__:metrics:39 - Key:Reddit_Reddit_anger_1--bleu :0.18716731595873118--bleu_4:0.18717024084885148--diversity:0.7952776168215064--novelty:0.8772981963923815--meteor:0.13964258453556538\n",
      "2021-08-11 18:21:32.207 | INFO     | __main__:metrics:39 - Key:Reddit_Reddit_sadness_0--bleu :0.16858535825164075--bleu_4:0.16858535825164075--diversity:0.7809108277084241--novelty:0.8817657691109974--meteor:0.16170848827884668\n",
      "2021-08-11 19:48:14.561 | INFO     | __main__:metrics:39 - Key:Reddit_Reddit_fear_0--bleu :0.16529066626954123--bleu_4:0.16529066626954123--diversity:0.7567048311240059--novelty:0.8806601379136259--meteor:0.1673527289617203\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/adarsh-binny/HULK_new/Counterspeech/metrics_results/Reddit.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-562649cb15f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reddit'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Reddit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b84581d54223>\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(trained_on, tested_on)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Key:{key}--bleu :{bleu}--bleu_4:{bleu_4}--diversity:{diversity}--novelty:{novelty}--meteor:{meteor_}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mtested_on\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/adarsh-binny/HULK_new/Counterspeech/metrics_results/Reddit.json'"
     ]
    }
   ],
   "source": [
    "metrics('Reddit','Reddit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****CONAN****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 20:05:30.253 | INFO     | __main__:metrics:39 - Key:CONAN_CONAN_fear_0--bleu :0.3628025900977262--bleu_4:0.3628025900977262--diversity:0.7905926672229553--novelty:0.8762991525843769--meteor:0.18265533594843028\n",
      "2021-08-11 20:09:17.151 | INFO     | __main__:metrics:39 - Key:CONAN_CONAN_joy_1--bleu :0.38790688956093866--bleu_4:0.38790688956093866--diversity:0.8103381803436333--novelty:0.877103428085856--meteor:0.17037657143026955\n",
      "2021-08-11 20:13:38.460 | INFO     | __main__:metrics:39 - Key:CONAN_CONAN_sadness_0--bleu :0.36060291577376724--bleu_4:0.36060291577376724--diversity:0.817835742407302--novelty:0.8833927203844146--meteor:0.17052843278504146\n",
      "2021-08-11 20:15:52.122 | INFO     | __main__:metrics:39 - Key:CONAN_CONAN_toxic_1--bleu :0.5009974364496044--bleu_4:0.5009974364496044--diversity:0.8087049867720634--novelty:0.84259901060552--meteor:0.1670292599359696\n",
      "2021-08-11 20:18:12.730 | INFO     | __main__:metrics:39 - Key:CONAN_CONAN_joy_0--bleu :0.5024940427181879--bleu_4:0.5025128542329175--diversity:0.7966439992002184--novelty:0.8368270669465472--meteor:0.17637176053283168\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/adarsh-binny/HULK_new/Counterspeech/metrics_results/CONAN.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8d687e8c3365>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CONAN'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'CONAN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-b84581d54223>\u001b[0m in \u001b[0;36mmetrics\u001b[0;34m(trained_on, tested_on)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Key:{key}--bleu :{bleu}--bleu_4:{bleu_4}--diversity:{diversity}--novelty:{novelty}--meteor:{meteor_}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mtested_on\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/adarsh-binny/HULK_new/Counterspeech/metrics_results/CONAN.json'"
     ]
    }
   ],
   "source": [
    "metrics('CONAN','CONAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Gab****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-14 09:58:14.966 | INFO     | __main__:metrics:44 - Key:Gab_Gab_joy_1--bleu :0.2406612273217836--bleu_4:0.2406612273217836--diversity:0.7663916699929321--novelty:0.8349554330589354--meteor:0.19452261662618517\n"
     ]
    }
   ],
   "source": [
    "metrics('Gab','Gab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load 'Hate-speech-CNERG/bert-base-uncased-hatexplain'. Make sure that:\n\n- 'Hate-speech-CNERG/bert-base-uncased-hatexplain' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'Hate-speech-CNERG/bert-base-uncased-hatexplain' is the correct path to a directory containing a 'config.json' file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/test_env/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mconfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dict_from_json_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_config_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-58675c5ffbf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hate-speech-CNERG/bert-base-uncased-hatexplain\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/adarsh-binny/HULK_new/Saved_models/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hate-speech-CNERG/bert-base-uncased-hatexplain\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/adarsh-binny/HULK_new/Saved_models'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/test_env/lib/python3.7/site-packages/transformers/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"bert-base-japanese\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/test_env/lib/python3.7/site-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \"\"\"\n\u001b[1;32m    186\u001b[0m         config_dict, _ = PretrainedConfig.get_config_dict(\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_config_archive_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mALL_PRETRAINED_CONFIG_ARCHIVE_MAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         )\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/test_env/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m                     )\n\u001b[1;32m    269\u001b[0m                 )\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load 'Hate-speech-CNERG/bert-base-uncased-hatexplain'. Make sure that:\n\n- 'Hate-speech-CNERG/bert-base-uncased-hatexplain' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'Hate-speech-CNERG/bert-base-uncased-hatexplain' is the correct path to a directory containing a 'config.json' file\n\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain\",cache_dir = '/home/adarsh-binny/HULK_new/Saved_models/')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain\",cache_dir = '/home/adarsh-binny/HULK_new/Saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('He is a great guy\", return_tensors=\"pt\")\n",
    "prediction_logits, _ = model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Toxicity_Score****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toxicity(trained_on,tested_on):\n",
    "    train_path = glob.glob(path_datasets+'/*'+trained_on+'*/*rain*')[0]\n",
    "    gen        = glob.glob(path_result+'/*'+trained_on+'*'+tested_on+'*1628*')\n",
    "    ref        = glob.glob(path_result+'/*'+tested_on+'*'+'references'+'*')[0]\n",
    "    \n",
    "#     print(gen,ref)\n",
    "    with open(ref, 'r') as file:\n",
    "        ref_dict   = json.loads(file.read())\n",
    "        \n",
    "    scores = {}\n",
    "    for files in gen:\n",
    "        with open(files, 'r') as file:\n",
    "            gen_dict  = json.loads(file.read())\n",
    "        emotion   = gen_dict['params']['task_name'][0][1]\n",
    "        gpu_id    = gen_dict['params']['gpu_id']\n",
    "        \n",
    "        detox = 0.0\n",
    "        tot   = 0.0\n",
    "        \n",
    "        for key in gen_dict['samples']:\n",
    "            for sentence in gen_dict['samples'][key]['counterspeech_model']:\n",
    "                detox += get_non_toxicity_score(sentence)\n",
    "                tot   += 1\n",
    "        \n",
    "        detox /= tot\n",
    "        key = trained_on+'_'+tested_on+'_'+emotion+'_'+str(gpu_id)\n",
    "        logger.info(f'Key -> {Key}-- Detox -> {detox}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity('Reddit','Reddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity('Gab','Gab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity('CONAN','CONAN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12990\n"
     ]
    }
   ],
   "source": [
    "trained_on = 'Reddit'\n",
    "tested_on  = 'Reddit'\n",
    "gen        = glob.glob(path_result+'/*'+trained_on+'*'+tested_on+'*1628*')\n",
    "ref        = glob.glob(path_result+'/*'+tested_on+'*'+'references'+'*')[0]\n",
    "train_path = glob.glob(path_datasets+'/*'+trained_on+'*/*rain*')[0]    \n",
    "\n",
    "#     print(gen,ref)\n",
    "with open(ref, 'r') as file:\n",
    "    ref_dict   = json.loads(file.read())\n",
    "    \n",
    "import time\n",
    "scores = {}\n",
    "from multiprocessing import Pool\n",
    "def f(x):\n",
    "    files = gen[x]\n",
    "    with open(files, 'r') as file:\n",
    "        gen_dict  = json.loads(file.read())\n",
    "    emotion   = gen_dict['params']['task_name'][0][1]\n",
    "    gpu_id    = gen_dict['params']['gpu_id']\n",
    "    hypo = []\n",
    "    refs = []\n",
    "    for key in gen_dict['samples']:\n",
    "        for sentences in gen_dict['samples'][key]['counterspeech_model']:\n",
    "            hypo.append(sentences)\n",
    "            refs.append(ref_dict['samples'][key]['counterspeech_model'])\n",
    "    print(len(hypo))\n",
    "\n",
    "f(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-test_env]",
   "language": "python",
   "name": "conda-env-.conda-test_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
