{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LPJOXuaa_5B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import itertools\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "from scipy.stats import norm\n",
        "import nltk.data\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import reuters\n",
        "from nltk. corpus import gutenberg\n",
        "from nltk.corpus import brown\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from gensim.models import KeyedVectors\n",
        "from keras.layers import Input, Dense, Lambda, Layer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras import metrics\n",
        "from gensim import models\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtO1owPZbu69",
        "colab_type": "text"
      },
      "source": [
        "### Load Google News Word2Vec model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O6xNlHpbwdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "b27d75a1-fbcc-49f0-962b-f58ba31275f4"
      },
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
        "EMBEDDING_FILE = '/root/input/GoogleNews-vectors-negative300.bin.gz' # from above\n",
        "word2vec = models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-07 05:42:19--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.187.21\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.187.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1647046227 (1.5G) [application/x-gzip]\n",
            "Saving to: ‘/root/input/GoogleNews-vectors-negative300.bin.gz’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   1.53G  68.0MB/s    in 22s     \n",
            "\n",
            "2020-08-07 05:42:42 (71.2 MB/s) - ‘/root/input/GoogleNews-vectors-negative300.bin.gz’ saved [1647046227/1647046227]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP8wu7WXb05I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oqt7ewIEc8tF",
        "colab_type": "text"
      },
      "source": [
        "### Functions to process sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev_dZtZfdDp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_into_sent (text):\n",
        "    strg = ''\n",
        "    for word in text:\n",
        "        strg += word\n",
        "        strg += ' '\n",
        "    strg_cleaned = strg.lower()\n",
        "    for x in ['\\xd5d','\\n','\"',\"!\", '#','$','%','&','(',')','*','+',',','-','/',':',';','<','=','>','?','@','[','^',']','_','`','{','|','}','~','\\t']:\n",
        "        strg_cleaned = strg_cleaned.replace(x, '')\n",
        "    sentences = sent_tokenize(strg_cleaned)\n",
        "    return sentences\n",
        "\n",
        "\n",
        "# In[4]:\n",
        "\n",
        "\n",
        "def vectorize_sentences(sentences):\n",
        "    vectorized = []\n",
        "    for sentence in sentences:\n",
        "        byword = sentence.split()\n",
        "        concat_vector = []\n",
        "        for word in byword:\n",
        "            try:\n",
        "                concat_vector.append(w2v[word])\n",
        "            except:\n",
        "                pass\n",
        "        vectorized.append(concat_vector)\n",
        "    return vectorized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcvmhNkdde1g",
        "colab_type": "text"
      },
      "source": [
        "### Import Data File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60PsAcDAdjV6",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "outputId": "2dfb095b-a39c-4301-d61f-6965501c7178"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4a8a0421-3a9b-476e-8d28-c4bb8a5ef9c4\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4a8a0421-3a9b-476e-8d28-c4bb8a5ef9c4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving reddit.csv to reddit.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5hPjt1Je5Wu",
        "colab_type": "text"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thWXVzWce8PZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "#Data Input and Preprocessing which is data specific\n",
        "data = pd.read_csv('reddit.csv')\n",
        "data_mod = pd.DataFrame([[]])\n",
        "\n",
        "for row in range(len(data[\"id\"])):\n",
        "    #print(data[\"id\"][row])\n",
        "    data[\"id\"][row]=(data[\"id\"][row].splitlines())\n",
        "    for num in range(len(data[\"id\"][row])):\n",
        "        strin=''.join(data[\"id\"][row][num].split())\n",
        "        newstr='';\n",
        "        for i in range(len(strin)-2):\n",
        "            newstr+=strin[i+2]\n",
        "        data[\"id\"][row][num]=newstr\n",
        "\n",
        "for row in range(len(data[\"hate_speech_idx\"])):\n",
        "        if data[\"hate_speech_idx\"][row]=='n/a':\n",
        "            data[\"hate_speech_idx\"][row]=\"[0]\"\n",
        "\n",
        "for row in range(len(data[\"hate_speech_idx\"])):\n",
        "        if data[\"hate_speech_idx\"][row]=='n/a':\n",
        "            data[\"hate_speech_idx\"][row]=\"[0]\"\n",
        "\n",
        "for row in range(len(data[\"hate_speech_idx\"])):\n",
        "        if isinstance(data[\"hate_speech_idx\"][row],float):\n",
        "            data[\"hate_speech_idx\"][row]=\"[0]\"  \n",
        "\n",
        "for row in range(len(data[\"response\"])):\n",
        "        if isinstance(data[\"response\"][row],float):\n",
        "            data[\"response\"][row]='This is normal'\n",
        "            \n",
        "for row in range(len(data[\"text\"])):\n",
        "    #print(data[\"id\"][row])\n",
        "    data[\"text\"][row]=(data[\"text\"][row].splitlines())\n",
        "    for num in range(len(data[\"text\"][row])):\n",
        "        strin=' '.join(data[\"text\"][row][num].split())\n",
        "        newstr='';\n",
        "        for i in range(len(strin)-2):\n",
        "            newstr+=strin[i+2]\n",
        "        data[\"text\"][row][num]=newstr\n",
        "    \n",
        "    data[\"hate_speech_idx\"][row]=data[\"hate_speech_idx\"][row].strip('][').split(', ');\n",
        "    data[\"hate_speech_idx\"][row]=list(map(int, data[\"hate_speech_idx\"][row]))\n",
        "    data[\"response\"][row] = data[\"response\"][row].replace(\"\\\"\",\"\\'\")\n",
        "    data[\"response\"][row] = data[\"response\"][row].replace(\"[\\'\",\"\")\n",
        "    data[\"response\"][row] = data[\"response\"][row].replace(\"\\']\",\"\")\n",
        "    data[\"response\"][row] = data[\"response\"][row].split('\\', \\'');\n",
        "    \n",
        "    for idx in data[\"hate_speech_idx\"][row]:\n",
        "        if idx!=0 and idx<=len(data[\"text\"][row]):\n",
        "            bad=idx-1;\n",
        "#            print(bad)\n",
        "            for resp in data[\"response\"][row]:\n",
        "                newrow=pd.Series([data[\"text\"][row][bad],resp])\n",
        "                rowdf=pd.DataFrame([newrow])\n",
        "                data_mod=pd.concat([data_mod,rowdf],ignore_index=True)\n",
        "                #print(\"done\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7M_Fu2EfJs2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "ca8c762d-a448-4310-ac21-169b568c4202"
      },
      "source": [
        "df=data_mod\n",
        "df = df.iloc[1:]\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A subsection of retarded Hungarians? Ohh boy....</td>\n",
              "      <td>I don't see a reason why it's okay to insult o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A subsection of retarded Hungarians? Ohh boy....</td>\n",
              "      <td>Language people. You can express your ire resp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A subsection of retarded Hungarians? Ohh boy....</td>\n",
              "      <td>Stop using mental impairments for insults on p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ah a liberal has slipped in. you can tell bec...</td>\n",
              "      <td>Persons with disabilities is the accepted term...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ah a liberal has slipped in. you can tell bec...</td>\n",
              "      <td>Avert from using mental impairments as a way t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   0                                                  1\n",
              "1   A subsection of retarded Hungarians? Ohh boy....  I don't see a reason why it's okay to insult o...\n",
              "2   A subsection of retarded Hungarians? Ohh boy....  Language people. You can express your ire resp...\n",
              "3   A subsection of retarded Hungarians? Ohh boy....  Stop using mental impairments for insults on p...\n",
              "4   ah a liberal has slipped in. you can tell bec...  Persons with disabilities is the accepted term...\n",
              "5   ah a liberal has slipped in. you can tell bec...  Avert from using mental impairments as a way t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuluVKutfZaW",
        "colab_type": "text"
      },
      "source": [
        "##SPLIT DATA INTO TRAIN AND TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0X74Isxfd-i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f38942e-4c4a-47f2-a757-f5728495d90f"
      },
      "source": [
        "data_train, data_test = train_test_split(df, test_size=0.20, random_state=42)\n",
        "train_x = pd.DataFrame(data_train[0])\n",
        "train_y = pd.DataFrame(data_train[1])\n",
        "test_x  = pd.DataFrame(data_test[0])\n",
        "test_y  = pd.DataFrame(data_test[1])\n",
        "type(test_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WEo4XadfsPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "a1a70326-e56e-44e6-ec2e-cff146c050bf"
      },
      "source": [
        "import nltk\n",
        "import re\n",
        "import os\n",
        "import collections\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def remove_punct(text):\n",
        "    text_nopunct = ''\n",
        "    text_nopunct = re.sub('['+string.punctuation+']', '', text)\n",
        "    return text_nopunct\n",
        "\n",
        "train_x[0] = train_x[0].apply(lambda x: remove_punct(x))\n",
        "train_y[1] = train_y[1].apply(lambda x: remove_punct(x))\n",
        "test_x[0] = test_x[0].apply(lambda x: remove_punct(x))\n",
        "test_y[1] = test_y[1].apply(lambda x: remove_punct(x))\n",
        "print(test_y.head())\n",
        "\n",
        "\n",
        "from nltk import word_tokenize, WordNetLemmatizer\n",
        "trainx_tokens = [word_tokenize(sen) for sen in train_x[0]]\n",
        "trainy_tokens = [word_tokenize(sen) for sen in train_y[1]]\n",
        "testx_tokens = [word_tokenize(sen) for sen in test_x[0]]\n",
        "testy_tokens = [word_tokenize(sen) for sen in test_y[1]]\n",
        "\n",
        "def lower_token(tokens): \n",
        "    return [w.lower() for w in tokens]    \n",
        "    \n",
        "lower_tokensx = [lower_token(token) for token in trainx_tokens]\n",
        "lower_tokensy = [lower_token(token) for token in trainy_tokens]\n",
        "\n",
        "print((lower_tokensy[0]))\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stoplist = stopwords.words('english')\n",
        "\n",
        "def remove_stop_wordsy(tokens):\n",
        "    return [word for word in trainy_tokens if word not in stoplist]\n",
        "\n",
        "def remove_stop_wordsx(tokens):\n",
        "    return [word for word in trainx_tokens if word not in stoplist]\n",
        "\n",
        "filtered_wordsx = [remove_stop_wordsx(sen) for sen in lower_tokensx] \n",
        "filtered_wordsy= [remove_stop_wordsy(sen) for sen in lower_tokensy] \n",
        "\n",
        "data_train_x=[[]]\n",
        "data_train_y=[[]]\n",
        "\n",
        "for i in range(len(filtered_wordsx[0])):\n",
        "    x=filtered_wordsx[0][i]\n",
        "    data_train_x.append(x)\n",
        "for i in range(len(filtered_wordsy[0])):\n",
        "    x=filtered_wordsy[0][i]\n",
        "    data_train_y.append(x)\n",
        "\n",
        "data_train_y.remove([])\n",
        "data_train_x.remove([])\n",
        "print((data_train_y[0]))\n",
        "\n",
        "resultx = [' '.join(sen) for sen in data_train_x]\n",
        "resulty = [' '.join(sen) for sen in data_train_y]\n",
        "#print(len(resultx))\n",
        "print(resulty[0])\n",
        "train_x = pd.DataFrame(resultx,columns=['Text'])\n",
        "train_y = pd.DataFrame(resulty,columns=['Response'])\n",
        "print(train_y.head())\n",
        "print(train_x.head())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "                                                       1\n",
            "4451   Your post uses speech that denigrates sex and ...\n",
            "7855   I really hope you know the history that goes i...\n",
            "7051   Immigrants and muslims are humans and are not ...\n",
            "12869  Retard is a pejorative and hateful word that d...\n",
            "7243   Well men and women can be moody but neither ar...\n",
            "['consider', 'using', 'different', 'language', 'in', 'the', 'future', 'that', 'does', 'not', 'undermine', 'your', 'points', 'with', 'hatred']\n",
            "['Consider', 'using', 'different', 'language', 'in', 'the', 'future', 'that', 'does', 'not', 'undermine', 'your', 'points', 'with', 'hatred']\n",
            "Consider using different language in the future that does not undermine your points with hatred\n",
            "                                            Response\n",
            "0  Consider using different language in the futur...\n",
            "1  Lift your conversation out of the gutter you c...\n",
            "2                    Stop using gender based insults\n",
            "3  Cts Reded and rerds are each offensive unneces...\n",
            "4  I would love to see your moms reaction to that...\n",
            "                                                Text\n",
            "0  Yes I wish the cunt would leave the US to beco...\n",
            "1  Everyone who disagrees with your libtard virtu...\n",
            "2  I hate entitled cunts as much as any of you an...\n",
            "3  Calling them cunts implies warmth and depth th...\n",
            "4              Would you take up for that nasty cunt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxsv9lNRlPeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "32a33d3e-c389-4536-de4b-dbfe6faf5f0e"
      },
      "source": [
        "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
        "    if len(tokens_list)<1:\n",
        "        return np.zeros(k)\n",
        "    if generate_missing:\n",
        "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
        "    else:\n",
        "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
        "    length = len(vectorized)\n",
        "    summed = np.sum(vectorized, axis=0)\n",
        "    averaged = np.divide(summed, length)\n",
        "    return averaged\n",
        "\n",
        "def get_word2vec_embeddings(vectors, clean_comments, generate_missing=False):\n",
        "    embeddings = clean_comments['tokens'].apply(lambda x: get_average_word2vec(x, vectors, \n",
        "                                                                                generate_missing=generate_missing))\n",
        "    return list(embeddings)\n",
        "\n",
        "train_x = []\n",
        "\n",
        "#print(text[0:2])\n",
        "import tensorflow as tf\n",
        "vect = vectorize_sentences(resultx)\n",
        "p=0\n",
        "data=[]\n",
        "y=np.zeros((300),dtype=float)\n",
        "print(len(vect[0]))\n",
        "for x in vect:\n",
        "    if len(x)<50:\n",
        "        t=50-len(x)\n",
        "        for i in range(t):\n",
        "            x.append(y)\n",
        "    elif len(x)>50:\n",
        "        x=x[0:50]\n",
        "    if(len(x)==50):\n",
        "        data.append(x)\n",
        "    p=p+1\n",
        "    if p==500:\n",
        "        break\n",
        "for x in data:\n",
        "    y=(list(itertools.chain.from_iterable(x)))\n",
        "    train_x.append(y)\n",
        "print(len(train_x[0]))\n",
        "\n",
        "train_y = []\n",
        "vect = vectorize_sentences(resulty)\n",
        "p=0\n",
        "data=[]\n",
        "y=np.zeros((300),dtype=float)\n",
        "print(len(vect[0]))\n",
        "for x in vect:\n",
        "    if len(x)<50:\n",
        "        t=50-len(x)\n",
        "        for i in range(t):\n",
        "            x.append(y)\n",
        "    elif len(x)>50:\n",
        "        x=x[0:50]\n",
        "    if(len(x)==50):\n",
        "        data.append(x)\n",
        "    p=p+1\n",
        "    if p==500:\n",
        "        break\n",
        "for x in data:\n",
        "    y=(list(itertools.chain.from_iterable(x)))\n",
        "    train_y.append(y)\n",
        "print(len(train_y[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "15000\n",
            "0\n",
            "15000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSvx6pejpJG7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "5f476d15-d2ec-4d72-d0a7-145f645a5901"
      },
      "source": [
        "print((train_x[0][0]))\n",
        "print((train_y[0][0]))\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)\n",
        "print(len(train_x))\n",
        "print(len(train_y))\n",
        "text_x=train_x[0:10]\n",
        "test_y=train_y[0:10]\n",
        "print(test_x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "0.0\n",
            "500\n",
            "500\n",
            "                                                       0\n",
            "4451    RToronto banned me for supporting free speech...\n",
            "7855                             Ur a niggerfaggot harry\n",
            "7051    Goddamn this is retarded Listen I know that y...\n",
            "12869                           LMAO a retard and a cuck\n",
            "7243                                Most women are cunts\n",
            "...                                                  ...\n",
            "11390   Thats just retard blaming not victim blaming ...\n",
            "950     Dont insult retards like that even they have ...\n",
            "5563                    Save this shit for ifunny faggot\n",
            "7915    Im convinced these have to be leftist shit po...\n",
            "8595                                  What a gradeA cunt\n",
            "\n",
            "[2858 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfp1ik2H1_t4",
        "colab_type": "text"
      },
      "source": [
        "### VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOog91-Q2Byb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 500\n",
        "original_dim = 3000\n",
        "latent_dim = 1000\n",
        "intermediate_dim = 1200\n",
        "epochs = 200\n",
        "epsilon_std = 1.0\n",
        "\n",
        "x = Input(batch_shape=(batch_size, original_dim))\n",
        "h = Dense(intermediate_dim, activation='relu')(x)\n",
        "z_mean = Dense(latent_dim)(h)\n",
        "z_log_var = Dense(latent_dim)(h)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n",
        "                              stddev=epsilon_std)\n",
        "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "decoder_h = Dense(intermediate_dim, activation='relu')\n",
        "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
        "h_decoded = decoder_h(z)\n",
        "x_decoded_mean = decoder_mean(h_decoded)\n",
        "\n",
        "# placeholder loss\n",
        "def zero_loss(y_true, y_pred):\n",
        "    return K.zeros_like(y_pred)\n",
        "\n",
        "# Custom loss layer\n",
        "class CustomVariationalLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.is_placeholder = True\n",
        "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def vae_loss(self, x, x_decoded_mean):\n",
        "        xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
        "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "        return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        x_decoded_mean = inputs[1]\n",
        "        loss = self.vae_loss(x, x_decoded_mean)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        # we don't use this output, but it has to have the correct shape:\n",
        "        return K.ones_like(x)\n",
        "\n",
        "loss_layer = CustomVariationalLayer()([x, x_decoded_mean])\n",
        "vae = Model(x, [loss_layer])\n",
        "vae.compile(optimizer='rmsprop', loss=[zero_loss])\n",
        "\n",
        "\n",
        "#train\n",
        "vae.fit(train, train,\n",
        "        shuffle=True,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(test, test), callbacks=cp)\n",
        "\n",
        "# build a model to project inputs on the latent space\n",
        "encoder = Model(x, z_mean)\n",
        "\n",
        "# build a generator that can sample from the learned distribution\n",
        "decoder_input = Input(shape=(latent_dim,))\n",
        "_h_decoded = decoder_h(decoder_input)\n",
        "_x_decoded_mean = decoder_mean(_h_decoded)\n",
        "generator = Model(decoder_input, _x_decoded_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB18-RR73F5n",
        "colab_type": "text"
      },
      "source": [
        "### Generating texts from Latent Space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIbDYhG92j5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sent_parse(sentence, mat_shape):\n",
        "    data_concat = []\n",
        "    word_vecs = vectorize_sentences(sentence)\n",
        "    for x in word_vecs:\n",
        "        data_concat.append(list(itertools.chain.from_iterable(x)))\n",
        "    zero_matr = np.zeros(mat_shape)\n",
        "    zero_matr[0] = np.array(data_concat)\n",
        "    return zero_matr\n",
        "\n",
        "# input: original dimension sentence vector\n",
        "# output: text\n",
        "def print_sentence_with_w2v(sent_vect):\n",
        "    word_sent = ''\n",
        "    tocut = sent_vect\n",
        "    for i in range (int(len(sent_vect)/300)):\n",
        "        word_sent += w2v.most_similar(positive=[tocut[:300]], topn=1)[0][0]\n",
        "        word_sent += ' '\n",
        "        tocut = tocut[300:]\n",
        "    print(word_sent)\n",
        "\n",
        "\n",
        "# input: encoded sentence vector\n",
        "# output: encoded sentence vector in dataset with highest cosine similarity\n",
        "def find_similar_encoding(sent_vect):\n",
        "    all_cosine = []\n",
        "    for sent in sent_encoded:\n",
        "        result = 1 - spatial.distance.cosine(sent_vect, sent)\n",
        "        all_cosine.append(result)\n",
        "    data_array = np.array(all_cosine)\n",
        "    maximum = data_array.argsort()[-3:][::-1][1]\n",
        "    new_vec = sent_encoded[maximum]\n",
        "    return new_vec\n",
        "\n",
        "# input: two written sentences, VAE batch-size, dimension of VAE input\n",
        "# output: the function embeds the sentences in latent-space, and then prints their generated text representations\n",
        "# along with the text representations of several points in between them\n",
        "def sent_2_sent(sent1,sent2, batch, dim):\n",
        "    a = sent_parse([sent1], (batch,dim))\n",
        "    b = sent_parse([sent2], (batch,dim))\n",
        "    encode_a = encoder.predict(a, batch_size = batch)\n",
        "    encode_b = encoder.predict(b, batch_size = batch)\n",
        "    test_hom = hom_shortest(encode_a[0], encode_b[0], 5)\n",
        "    \n",
        "    for point in test_hom:\n",
        "        p = generator.predict(np.array([point]))[0]\n",
        "        print_sentence(p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwIfvU720HC",
        "colab_type": "text"
      },
      "source": [
        "###Sample_Texts_generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVPvEhJC27EN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_sentence_with_w2v(train_x[1])\n",
        "print_sentence_with_w2v(train_x[2])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}